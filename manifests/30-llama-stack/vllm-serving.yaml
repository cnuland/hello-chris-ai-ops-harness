# vLLM Model Serving - Plain Deployment
# Deploys ibm-granite/granite-4.0-h-tiny via vLLM on A100 GPU
# Uses RHAIIS vllm-cuda-rhel9 image with nvidia runtimeClass
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: granite-4-server
  namespace: llm-serving
  labels:
    app: granite-4-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: granite-4-server
  template:
    metadata:
      labels:
        app: granite-4-server
    spec:
      runtimeClassName: nvidia
      serviceAccountName: granite-sa
      tolerations:
        - key: nvidia.com/gpu
          operator: Equal
          value: "true"
          effect: NoSchedule
      containers:
        - name: vllm
          image: registry.redhat.io/rhaiis/vllm-cuda-rhel9@sha256:094db84a1da5e8a575d0c9eade114fa30f4a2061064a338e3e032f3578f8082a
          command:
            - /bin/bash
            - -c
          args:
            - |
              echo "Downloading model..."
              vllm serve ibm-granite/granite-4.0-h-tiny \
                --served-model-name granite-4 \
                --port 8080 \
                --max-model-len 8192 \
                --tensor-parallel-size 1 \
                --trust-remote-code \
                --disable-uvicorn-access-log \
                --dtype bfloat16 \
                --enable-auto-tool-choice \
                --tool-call-parser hermes
          env:
            - name: HOME
              value: /tmp
            - name: HF_HUB_CACHE
              value: /models
            - name: HF_HUB_OFFLINE
              value: "0"
            - name: TRANSFORMERS_OFFLINE
              value: "0"
            - name: VLLM_LOGGING_LEVEL
              value: INFO
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: llm-d-hf-token
                  key: HF_TOKEN
          ports:
            - containerPort: 8080
              protocol: TCP
          resources:
            limits:
              cpu: "4"
              memory: 32Gi
              nvidia.com/gpu: "1"
            requests:
              cpu: "4"
              memory: 32Gi
              nvidia.com/gpu: "1"
          volumeMounts:
            - mountPath: /dev/shm
              name: shm
            - mountPath: /models
              name: model-cache
      volumes:
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 12Gi
        - name: model-cache
          emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: granite-4-server
  namespace: llm-serving
  labels:
    app: granite-4-server
spec:
  selector:
    app: granite-4-server
  ports:
    - port: 8080
      targetPort: 8080
      protocol: TCP
      name: http
  type: ClusterIP
